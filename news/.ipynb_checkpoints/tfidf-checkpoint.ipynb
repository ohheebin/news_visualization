{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#전체 기사 내용에서 쓰이는 단어들의 tf-idf 값을 구해서 자주 쓰이는 쓸모없는 단어들을 수치로 표현해준다.\n",
    "import sys\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "import os\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf를 구하기 위해서 tf와 idf를 구하기 위한 시작 1글자 이상의 단어만\n",
    "# 생각해볼점 한글자인 것들을 제거 한후 할 것인지 아니면 1글자들도 tf-idf 를 구해서 제거해서 할 것인지\n",
    "# 제거 한 후 하는 것은 한 글자들은 대체로 의미가 없는 단어들이지만 필요한 단어가 있을 수도 있으며 좋은 점은 tf-idf 후 글자 clean의 시간을 단축시킴\n",
    "# 제거 하지 않으면 실제로 필요한 단어들도 존재 할 수 있으며 자주쓰이는 단어는 당연히 골라 낼 수 있을 것이다. 하지만 clean 시간이 길어 진다.\n",
    "def get_tags(text):\n",
    "    #spliter = Twitter()\n",
    "    spliter = Mecab()\n",
    "    nouns = spliter.nouns(text)\n",
    "    count = Counter(nouns)\n",
    "    return_list = []\n",
    "    for n, c in count.most_common():\n",
    "        if len(n) > 1:\n",
    "            temp = {'tag': n, 'count': c, 'dict_count' : 1}\n",
    "            return_list.append(temp)\n",
    "        #temp = {'tag': n, 'count': c, 'dict_count' : 1}\n",
    "        #return_list.append(temp)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf가 0에 가까울 수록 좋지 제거 대상\n",
    "#파라미터로는 각각의 단어에 대한 단어의 총 count와 어떤 문서에 쓰여져있는지의 count를 저장한 dict를 가져오고 총 문서의 수인 dict_num을 가져온다.\n",
    "#많은 문서에서 쓰이는 단어 일수록 idf 값이 0에 가까워 지며 그로 인해 tf-idf 값이 0에 가까워 지면 제거 대상이다.\n",
    "#로그빈도를 이용해서 tf값을 구했다. 값이 커지는 것을 막을 수 있다.\n",
    "def tfidf_func(result_dict, dict_num):\n",
    "    result = []\n",
    "    for tag in result_dict:\n",
    "        tf = math.log10(result_dict[tag][0] + 1)\n",
    "        idf = math.log10(dict_num / result_dict[tag][1])\n",
    "        tfidf = tf * idf\n",
    "        #print(tag,result_dict[tag][0],result_dict[tag][1],tfidf)\n",
    "        #tfidf 값을 조절해서 자주쓰이는 단어에 대해 알아낼수 있다.\n",
    "        if tfidf < 0.9:\n",
    "            result.append(tag)\n",
    "    #print(dict_num)\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf 하기\n",
    "#처음에는 각각의 문서에 쓰여져 있는 단어의 count와 문서 count값을 가져와서 diction에 저장하고 계속해서 \n",
    "def result_tfidf():\n",
    "    num = 0\n",
    "    #전체 문서에 쓰인 명사와 그 명사의 count, 명사의 문서에 쓰인 count 수를 저장하기 위한 변수\n",
    "    result_dict = {}\n",
    "    \n",
    "    for file in os.listdir(\"article_dict/20170807\"):\n",
    "        #존재 여부 확인\n",
    "        if file.endswith(\".txt\"):\n",
    "            open_text_file = open('article_dict/20170807/'+file, 'r')\n",
    "            text = open_text_file.read()\n",
    "            \n",
    "            #get_tags(text)를 통해 하나의 문서에 쓰인 명사와 그 명사의 count수 그리고 문서 count수인 1을 return 받는다.\n",
    "            tags = get_tags(text)\n",
    "            open_text_file.close()\n",
    "            \n",
    "            #전체적인 명사와 count, dict_count를 저장해 result_dict에 그 값들을 저장한다. \n",
    "            for tag in tags:\n",
    "                noun = tag['tag']\n",
    "                count = tag['count']\n",
    "                dict_count = tag['dict_count']\n",
    "                if noun in result_dict:\n",
    "                    result_dict[noun][0] += count\n",
    "                    result_dict[noun][1] += dict_count\n",
    "                else:\n",
    "                    result_dict[noun] = [count, dict_count]\n",
    "                    \n",
    "            #총 문서의 수를 저장하기 위해 사용하는 변수 하나의 문서를 처리할떄 마다 값을 1증가 시킨다.        \n",
    "            num = num + 1\n",
    "            print(file)\n",
    "    #위에서 계산한 result_dict와 전체 문서의 수인 num값을 파라미터로 넘겨준다.        \n",
    "    return tfidf_func(result_dict, num)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_20170807_0.txt\n",
      "article_20170807_1.txt\n",
      "article_20170807_10.txt\n",
      "article_20170807_100.txt\n",
      "article_20170807_1000.txt\n",
      "article_20170807_1001.txt\n",
      "article_20170807_1002.txt\n",
      "article_20170807_1003.txt\n",
      "article_20170807_1004.txt\n",
      "article_20170807_1005.txt\n",
      "article_20170807_1006.txt\n",
      "article_20170807_1007.txt\n",
      "article_20170807_1008.txt\n",
      "article_20170807_1009.txt\n",
      "article_20170807_101.txt\n",
      "article_20170807_1010.txt\n",
      "article_20170807_1011.txt\n",
      "article_20170807_1012.txt\n",
      "article_20170807_1013.txt\n",
      "article_20170807_1014.txt\n",
      "article_20170807_1015.txt\n",
      "article_20170807_1016.txt\n",
      "article_20170807_1017.txt\n",
      "article_20170807_1018.txt\n",
      "article_20170807_1019.txt\n",
      "article_20170807_102.txt\n",
      "article_20170807_1020.txt\n",
      "article_20170807_1021.txt\n",
      "article_20170807_1022.txt\n",
      "article_20170807_1023.txt\n",
      "article_20170807_1024.txt\n",
      "article_20170807_1025.txt\n",
      "article_20170807_1026.txt\n",
      "article_20170807_1027.txt\n",
      "article_20170807_1028.txt\n",
      "article_20170807_1029.txt\n",
      "article_20170807_103.txt\n",
      "article_20170807_1030.txt\n",
      "article_20170807_1031.txt\n",
      "article_20170807_1032.txt\n",
      "article_20170807_1033.txt\n",
      "article_20170807_1034.txt\n",
      "article_20170807_1035.txt\n",
      "article_20170807_1036.txt\n",
      "article_20170807_1037.txt\n",
      "article_20170807_1038.txt\n",
      "article_20170807_1039.txt\n",
      "article_20170807_104.txt\n",
      "article_20170807_1040.txt\n",
      "article_20170807_1041.txt\n",
      "article_20170807_1042.txt\n",
      "article_20170807_1043.txt\n",
      "article_20170807_1044.txt\n",
      "article_20170807_1045.txt\n",
      "article_20170807_1046.txt\n",
      "article_20170807_1047.txt\n",
      "article_20170807_1048.txt\n",
      "article_20170807_1049.txt\n",
      "article_20170807_105.txt\n",
      "article_20170807_1050.txt\n",
      "article_20170807_1051.txt\n",
      "article_20170807_1052.txt\n",
      "article_20170807_1053.txt\n",
      "article_20170807_1054.txt\n",
      "article_20170807_1055.txt\n",
      "article_20170807_1056.txt\n",
      "article_20170807_1057.txt\n",
      "article_20170807_1058.txt\n",
      "article_20170807_1059.txt\n",
      "article_20170807_106.txt\n",
      "article_20170807_1060.txt\n",
      "article_20170807_1061.txt\n",
      "article_20170807_1062.txt\n",
      "article_20170807_1063.txt\n",
      "article_20170807_1064.txt\n",
      "article_20170807_1065.txt\n",
      "article_20170807_1066.txt\n",
      "article_20170807_1067.txt\n",
      "article_20170807_1068.txt\n",
      "article_20170807_1069.txt\n",
      "article_20170807_107.txt\n",
      "article_20170807_1070.txt\n",
      "article_20170807_1071.txt\n",
      "article_20170807_1072.txt\n",
      "article_20170807_1073.txt\n",
      "article_20170807_1074.txt\n",
      "article_20170807_1075.txt\n",
      "article_20170807_1076.txt\n",
      "article_20170807_1077.txt\n",
      "article_20170807_1078.txt\n",
      "article_20170807_1079.txt\n",
      "article_20170807_108.txt\n",
      "article_20170807_1080.txt\n",
      "article_20170807_1081.txt\n",
      "article_20170807_1082.txt\n",
      "article_20170807_1083.txt\n",
      "article_20170807_1084.txt\n",
      "article_20170807_1085.txt\n",
      "article_20170807_1086.txt\n",
      "article_20170807_1087.txt\n",
      "article_20170807_1088.txt\n",
      "article_20170807_1089.txt\n",
      "article_20170807_109.txt\n",
      "article_20170807_1090.txt\n",
      "article_20170807_1091.txt\n",
      "article_20170807_1092.txt\n",
      "article_20170807_1093.txt\n",
      "article_20170807_1094.txt\n",
      "article_20170807_1095.txt\n",
      "article_20170807_1096.txt\n",
      "article_20170807_1097.txt\n",
      "article_20170807_1098.txt\n",
      "article_20170807_1099.txt\n",
      "article_20170807_11.txt\n",
      "article_20170807_110.txt\n",
      "article_20170807_1100.txt\n",
      "article_20170807_1101.txt\n",
      "article_20170807_1102.txt\n",
      "article_20170807_1103.txt\n",
      "article_20170807_1104.txt\n",
      "article_20170807_1105.txt\n",
      "article_20170807_1106.txt\n",
      "article_20170807_1107.txt\n",
      "article_20170807_1108.txt\n",
      "article_20170807_1109.txt\n",
      "article_20170807_111.txt\n",
      "article_20170807_1110.txt\n",
      "article_20170807_1111.txt\n",
      "article_20170807_1112.txt\n",
      "article_20170807_1113.txt\n",
      "article_20170807_1114.txt\n",
      "article_20170807_1115.txt\n",
      "article_20170807_1116.txt\n",
      "article_20170807_1117.txt\n",
      "article_20170807_1118.txt\n",
      "article_20170807_1119.txt\n",
      "article_20170807_112.txt\n",
      "article_20170807_1120.txt\n",
      "article_20170807_1121.txt\n",
      "article_20170807_1122.txt\n",
      "article_20170807_1123.txt\n",
      "article_20170807_1124.txt\n",
      "article_20170807_1125.txt\n",
      "article_20170807_1126.txt\n",
      "article_20170807_1127.txt\n",
      "article_20170807_1128.txt\n",
      "article_20170807_1129.txt\n",
      "article_20170807_113.txt\n",
      "article_20170807_1130.txt\n",
      "article_20170807_1131.txt\n",
      "article_20170807_1132.txt\n",
      "article_20170807_1133.txt\n",
      "article_20170807_1134.txt\n",
      "article_20170807_1135.txt\n",
      "article_20170807_1136.txt\n",
      "article_20170807_1137.txt\n",
      "article_20170807_1138.txt\n",
      "article_20170807_1139.txt\n",
      "article_20170807_114.txt\n",
      "article_20170807_1140.txt\n",
      "article_20170807_1141.txt\n",
      "article_20170807_1142.txt\n",
      "article_20170807_1143.txt\n",
      "article_20170807_1144.txt\n",
      "article_20170807_1145.txt\n",
      "article_20170807_1146.txt\n",
      "article_20170807_1147.txt\n",
      "article_20170807_1148.txt\n",
      "article_20170807_1149.txt\n",
      "article_20170807_115.txt\n",
      "article_20170807_1150.txt\n",
      "article_20170807_1151.txt\n",
      "article_20170807_1152.txt\n",
      "article_20170807_1153.txt\n",
      "article_20170807_1154.txt\n",
      "article_20170807_1155.txt\n",
      "article_20170807_1156.txt\n",
      "article_20170807_1157.txt\n",
      "article_20170807_1158.txt\n",
      "article_20170807_1159.txt\n",
      "article_20170807_116.txt\n",
      "article_20170807_1160.txt\n",
      "article_20170807_1161.txt\n",
      "article_20170807_1162.txt\n",
      "article_20170807_1163.txt\n",
      "article_20170807_1164.txt\n",
      "article_20170807_1165.txt\n",
      "article_20170807_1166.txt\n",
      "article_20170807_1167.txt\n",
      "article_20170807_1168.txt\n",
      "article_20170807_1169.txt\n",
      "article_20170807_117.txt\n",
      "article_20170807_1170.txt\n",
      "article_20170807_1171.txt\n",
      "article_20170807_1172.txt\n",
      "article_20170807_1173.txt\n",
      "article_20170807_1174.txt\n",
      "article_20170807_1175.txt\n",
      "article_20170807_1176.txt\n",
      "article_20170807_1177.txt\n",
      "article_20170807_1178.txt\n",
      "article_20170807_1179.txt\n",
      "article_20170807_118.txt\n",
      "article_20170807_1180.txt\n",
      "article_20170807_1181.txt\n",
      "article_20170807_1182.txt\n",
      "article_20170807_1183.txt\n",
      "article_20170807_1184.txt\n",
      "article_20170807_1185.txt\n",
      "article_20170807_1186.txt\n",
      "article_20170807_1187.txt\n",
      "article_20170807_1188.txt\n",
      "article_20170807_1189.txt\n"
     ]
    }
   ],
   "source": [
    "result_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf를 통해 얻어온 제거 대상을 clean해주는 함수.\n",
    "remove_list = '|'.join(result_tfidf())\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(remove_list, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#main function 각 문서들의 최종 clean된 명사들을 word2vec하기위해 하나의 txt파일에 저장한다.\n",
    "def main():\n",
    "    spliter = Twitter()\n",
    "    #입력 파일명 = file\n",
    "    for file in os.listdir(\"cleand_article_dict\"):\n",
    "        #존재 여부 확인\n",
    "        if file.endswith(\".txt\"):\n",
    "            #출력 파일명\n",
    "            output_file_name = 'nouns.txt'\n",
    "            #output_file_name = 'konlpy/konlpy'+str(num)+'.txt'\n",
    "            \n",
    "            open_text_file = open('cleand_article_dict/'+file, 'r')\n",
    "            text = open_text_file.read()\n",
    "            open_text_file.close()\n",
    "            \n",
    "            #open_output_file = open(output_file_name, 'w')\n",
    "            open_output_file = open(output_file_name, 'a+')\n",
    "            \n",
    "            result_text = clean_text(text)\n",
    "            nouns = spliter.nouns(result_text)\n",
    "            \n",
    "            for n in nouns:\n",
    "                if len(n) > 1:\n",
    "                    open_output_file.write(' ' + n)\n",
    "            open_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
