{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#전체 기사 내용에서 쓰이는 단어들의 tf-idf 값을 구해서 자주 쓰이는 쓸모없는 단어들을 수치로 표현해준다.\n",
    "import sys\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "import os\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf를 구하기 위해서 tf와 idf를 구하기 위한 시작 1글자 이상의 단어만\n",
    "# 생각해볼점 한글자인 것들을 제거 한후 할 것인지 아니면 1글자들도 tf-idf 를 구해서 제거해서 할 것인지\n",
    "# 제거 한 후 하는 것은 한 글자들은 대체로 의미가 없는 단어들이지만 필요한 단어가 있을 수도 있으며 좋은 점은 tf-idf 후 글자 clean의 시간을 단축시킴\n",
    "# 제거 하지 않으면 실제로 필요한 단어들도 존재 할 수 있으며 자주쓰이는 단어는 당연히 골라 낼 수 있을 것이다. 하지만 clean 시간이 길어 진다.\n",
    "def get_tags(text):\n",
    "    spliter = Twitter()\n",
    "    #spliter = Mecab()\n",
    "    nouns = spliter.nouns(text)\n",
    "    count = Counter(nouns)\n",
    "    return_list = []\n",
    "    for n, c in count.most_common():\n",
    "        if len(n) > 1:\n",
    "            temp = {'tag': n, 'count': c, 'dict_count' : 1}\n",
    "            return_list.append(temp)\n",
    "        #temp = {'tag': n, 'count': c, 'dict_count' : 1}\n",
    "        #return_list.append(temp)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf가 0에 가까울 수록 좋지 제거 대상\n",
    "#파라미터로는 각각의 단어에 대한 단어의 총 count와 어떤 문서에 쓰여져있는지의 count를 저장한 dict를 가져오고 총 문서의 수인 dict_num을 가져온다.\n",
    "#많은 문서에서 쓰이는 단어 일수록 idf 값이 0에 가까워 지며 그로 인해 tf-idf 값이 0에 가까워 지면 제거 대상이다.\n",
    "#로그빈도를 이용해서 tf값을 구했다. 값이 커지는 것을 막을 수 있다.\n",
    "def tfidf_func(result_dict, dict_num):\n",
    "    result = []\n",
    "    for tag in result_dict:\n",
    "        tf = math.log10(result_dict[tag][0] + 1)\n",
    "        idf = math.log10(dict_num / result_dict[tag][1])\n",
    "        tfidf = tf * idf\n",
    "        #print(tag,result_dict[tag][0],result_dict[tag][1],tfidf)\n",
    "        #tfidf 값을 조절해서 자주쓰이는 단어에 대해 알아낼수 있다.\n",
    "        if tfidf < 1.9 and result_dict[tag][0] > 10:\n",
    "            result.append(tag)\n",
    "            print(tag,result_dict[tag][0],result_dict[tag][1],tfidf)\n",
    "    #print(dict_num)\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf 하기\n",
    "#처음에는 각각의 문서에 쓰여져 있는 단어의 count와 문서 count값을 가져와서 diction에 저장하고 계속해서 \n",
    "def result_tfidf():\n",
    "    num = 0\n",
    "    #전체 문서에 쓰인 명사와 그 명사의 count, 명사의 문서에 쓰인 count 수를 저장하기 위한 변수\n",
    "    result_dict = {}\n",
    "    output_file_name = 'noun.txt'\n",
    "    for (path, dir , files) in os.walk(\"article_dict\"):\n",
    "        #존재 여부 확인\n",
    "        for fname in dir:\n",
    "            fullname = path+\"/\"+fname\n",
    "            #print(fullname)\n",
    "            for file in os.listdir(fullname):\n",
    "                #print(file)\n",
    "                if file.endswith(\".txt\"):\n",
    "                    open_text_file = open(fullname+\"/\"+file, 'r')\n",
    "                    text = open_text_file.read()\n",
    "            \n",
    "                    #get_tags(text)를 통해 하나의 문서에 쓰인 명사와 그 명사의 count수 그리고 문서 count수인 1을 return 받는다.\n",
    "                    tags = get_tags(text)\n",
    "                    open_text_file.close()\n",
    "            \n",
    "                    #전체적인 명사와 count, dict_count를 저장해 result_dict에 그 값들을 저장한다. \n",
    "                    for tag in tags:\n",
    "                        noun = tag['tag']\n",
    "                        count = tag['count']\n",
    "                        dict_count = tag['dict_count']\n",
    "                        \n",
    "                        open_output_file = open(output_file_name, 'a+')\n",
    "                        open_output_file.write(noun+'\\n')\n",
    "                        open_output_file.close()\n",
    "                        \n",
    "                        if noun in result_dict:\n",
    "                            result_dict[noun][0] += count\n",
    "                            result_dict[noun][1] += dict_count\n",
    "                        else:\n",
    "                            result_dict[noun] = [count, dict_count]\n",
    "                    \n",
    "                    #총 문서의 수를 저장하기 위해 사용하는 변수 하나의 문서를 처리할떄 마다 값을 1증가 시킨다.        \n",
    "                    num = num + 1\n",
    "    #위에서 계산한 result_dict와 전체 문서의 수인 num값을 파라미터로 넘겨준다.        \n",
    "    return tfidf_func(result_dict, num)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidf를 통해 얻어온 제거 대상을 clean해주는 함수.\n",
    "def clean_text(text,remove_list):\n",
    "    cleaned_text = re.sub(remove_list, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#main function 각 문서들의 최종 clean된 명사들을 word2vec하기위해 하나의 txt파일에 저장한다.\n",
    "def main():\n",
    "    spliter = Twitter()\n",
    "    remove_list = '|'.join(result_tfidf())\n",
    "    #입력 파일명 = file\n",
    "    for (path, dir , files) in os.walk(\"article_dict\"):\n",
    "        for fname in dir:\n",
    "            fullname = path+\"/\"+fname\n",
    "            #print(fullname)\n",
    "            for file in os.listdir(fullname):\n",
    "                #존재 여부 확인\n",
    "                if file.endswith(\".txt\"):\n",
    "                    #출력 파일명\n",
    "                    output_file_name = 'result.txt'\n",
    "                    #output_file_name = 'konlpy/konlpy'+str(num)+'.txt'\n",
    "                    #print(fullname+\"/\"+file)\n",
    "                    open_text_file = open(fullname+'/'+file, 'r')\n",
    "                    text = open_text_file.read()\n",
    "                    open_text_file.close()\n",
    "            \n",
    "                    #open_output_file = open(output_file_name, 'w')\n",
    "                    open_output_file = open(output_file_name, 'a+')\n",
    "            \n",
    "                    result_text = clean_text(text,remove_list)\n",
    "                    nouns = spliter.nouns(result_text)\n",
    "                    \n",
    "                    for n in nouns:\n",
    "                        if len(n) > 1:\n",
    "                            open_output_file.write(' ' + n)\n",
    "                    open_output_file.write('\\n')\n",
    "                    open_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 1783 789 1.122433205178343\n",
      "본문 3458 1747 0.0\n",
      "내용 3935 1747 0.0\n",
      "플레이어 3495 1747 0.0\n",
      "제보 935 523 1.556328190225612\n",
      "오류 1748 1747 0.0\n",
      "우회 1749 1747 0.0\n",
      "함수 1747 1747 0.0\n",
      "추가 2314 1747 0.0\n",
      "기자 2299 1462 0.26001462912987344\n",
      "오후 695 567 1.389211123984159\n",
      "현지 482 389 1.7508549381157668\n",
      "시간 900 619 1.3314056565644168\n",
      "지역 830 496 1.596470622524032\n",
      "안보 680 440 1.696602463862741\n",
      "포럼 448 351 1.848578007325813\n",
      "대표 3116 691 1.407329092531534\n",
      "최신 542 541 1.3922746691782986\n",
      "만화 310 310 1.8718915780353236\n",
      "영상 639 391 1.8243429141223726\n",
      "바로가기 426 338 1.8764846542363163\n",
      "여러분 461 452 1.5645564473545264\n",
      "클릭 612 609 1.2757526795767922\n",
      "무단 1079 1048 0.6732126552854238\n",
      "전재 947 947 0.7916611193123246\n",
      "배포 1056 1051 0.6673836803620238\n",
      "금지 1517 1104 0.6341032788355464\n",
      "북한 4102 605 1.6639688494571878\n",
      "서울 1558 775 1.1270465895233164\n",
      "위해 735 499 1.5601330017798478\n",
      "국회 775 544 1.4642756102633965\n",
      "대해 1093 621 1.3651305463703394\n",
      "대한 1174 608 1.4072725875790975\n",
      "페이스북 675 529 1.4682817131119723\n",
      "정부 1396 464 1.8109252230417674\n",
      "통해 642 482 1.5704803795532465\n",
      "라며 600 376 1.8538012215314439\n",
      "제재 2191 479 1.8774100410183696\n",
      "유엔 959 412 1.8710641148880185\n",
      "대북 1480 473 1.799073650492977\n",
      "연합뉴스 690 483 1.5854105687742643\n",
      "문제 819 432 1.768128930823275\n",
      "한국 1178 468 1.7570504171018475\n",
      "이번 1013 477 1.694727623044728\n",
      "유행 299 299 1.8990149486644485\n",
      "트렌드 387 387 1.6946024931798567\n",
      "집결 303 302 1.8926596782695955\n",
      "의원 1668 555 1.6047830038643474\n",
      "출마 1262 430 1.8882101872795367\n",
      "국민의당 1477 595 1.4826974355224578\n",
      "여의도 370 341 1.8230697765465365\n",
      "방안 561 382 1.815457148444771\n",
      "지난 638 403 1.7870699846172122\n",
      "사진 760 404 1.8323057540514682\n",
      "오전 372 334 1.8478922211099782\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
