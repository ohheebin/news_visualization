{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bc925eb335a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymysql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymysql'"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from konlpy.tag import Twitter\n",
    "import urllib.request\n",
    "import sys\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host = 'localhost',\n",
    "                      user = 'root',\n",
    "                      password = '140508',\n",
    "                      db = 'news',\n",
    "                      charset = 'utf8')\n",
    "curs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#크롤링함수 url의 본문 내용 저장\n",
    "def get_text(URL):\n",
    "    source_code_from_URL = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='articleBodyContents'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text저장 함수\n",
    "# url을 get_text(url)을 통해 기사의 내용을 가져온후 clean_text 함수를 통해 clean_text로 \n",
    "# 변경시켜 준후 명사화 시켜서 저장 \n",
    "def article_text(URL, num, day):\n",
    "    spliter = Twitter()\n",
    "    #spliter = Mecab()\n",
    "    try:\n",
    "        OUTPUT_FILE_NAME = 'article_dict/'+ day +'/article_'+ day +'_'+ str(num) +'.txt'\n",
    "        open_output_file = open(OUTPUT_FILE_NAME, 'w')\n",
    "        article_nouns = spliter.nouns(get_text(URL))\n",
    "        for n in article_nouns: \n",
    "            if len(n) > 1 :\n",
    "                open_output_file.write(' ' + n)\n",
    "        open_output_file.close()\n",
    "    finally:\n",
    "        open_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#db에 저장하는 함수\n",
    "def db_manager(url, title, day):\n",
    "    sql = \"insert into data (url, title, date) values(%s,%s,%s)\"\n",
    "    curs.execute(sql, (url, title, day))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input ex) 20170808. 날짜 입력해서 crawling하기\n",
    "def main(day):\n",
    "    page = 1\n",
    "    temp_page = 0\n",
    "    #db에 저장 시킬 id\n",
    "    db_id = 0\n",
    "    first_url = ''\n",
    "    \n",
    "    while 1 == 1:\n",
    "        url = 'http://news.naver.com/main/list.nhn?sid1=100&listType=title&mid=sec&mode=LSD&date='+str(day)+'&page='+str(page)\n",
    "        source_code = requests.get(url)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text,'lxml')\n",
    "        \n",
    "        for body in soup.find_all(\"a\", class_=\"nclicks(fls.list)\"):\n",
    "             \n",
    "            #db에 저장 시킬 url\n",
    "            url = body.get('href')\n",
    "            \n",
    "            #마지막 페이지에 도달할시 함수 종료\n",
    "            if first_url == url:\n",
    "                return\n",
    "            \n",
    "            #db에 저장 시킬 article title\n",
    "            title = str(body.find_all(text=True))\n",
    "            #db에 저장 시킬 명사화된 article title 하지만 넣을지 말지 고민....\n",
    "            #nouns_title = clean_text(str(spliter.nouns(title)))\n",
    "            \n",
    "            #마지막 페이지를 구별하기 위해 만듬\n",
    "            if temp_page != page:\n",
    "                first_url = url\n",
    "                temp_page += 1\n",
    "            \n",
    "            #db에 저장시킬 함수 호출\n",
    "            db_manager(url, title, str(day))\n",
    "            \n",
    "            print(url)\n",
    "            #print(title)\n",
    "            #날짜 폴더가 생성되있어야 한다. 본문내용 저장 함수 호출\n",
    "            article_text(url,db_id,str(day))\n",
    "            \n",
    "            db_id += 1\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
