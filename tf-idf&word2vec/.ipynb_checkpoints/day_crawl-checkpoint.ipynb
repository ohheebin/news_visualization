{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "from konlpy.tag import Twitter\n",
    "import urllib.request\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#크롤링함수 url의 본문 내용 저장\n",
    "def get_text(URL):\n",
    "    source_code_from_URL = urllib.request.urlopen(URL)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='articleBodyContents'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#클리닝함수 본문 내용 클리닝\n",
    "def clean_text(text):   \n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;▶“’ⓒ:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]', '', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text저장 함수\n",
    "# url을 get_text(url)을 통해 기사의 내용을 가져온후 clean_text 함수를 통해 clean_text로 \n",
    "# 변경시켜 준후 명사화 시켜서 저장 \n",
    "def article_text(URL, num, day):\n",
    "    spliter = Twitter()\n",
    "    try:\n",
    "        OUTPUT_FILE_NAME = 'article_dict/'+ day +'/article_'+ day +'_'+ str(num) +'.txt'\n",
    "        open_output_file = open(OUTPUT_FILE_NAME, 'w')\n",
    "        result_text = clean_text(get_text(URL))\n",
    "        article_nouns = spliter.nouns(result_text)\n",
    "        for n in article_nouns: \n",
    "            open_output_file.write(' ' + n)\n",
    "        open_output_file.close()\n",
    "        \n",
    "    finally:\n",
    "        open_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#db에 저장하는 함수\n",
    "def db_manager(db_id, url, title, day):\n",
    "    \"\"\"\n",
    "    db에 저장시킬 sql문 작성\n",
    "    명사화된 title저장\n",
    "    INSERT INTO db_article VALUES('db_id','url','title','nouns_title','day');\n",
    "    명사화된 title저장안하기\n",
    "    INSERT INTO db_article VALUES('db_id','url','title','day');\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input ex) 20170808. 날짜 입력해서 crawling하기\n",
    "def main(day):\n",
    "    page = 1\n",
    "    temp_page = 0\n",
    "    #db에 저장 시킬 id\n",
    "    db_id = 0\n",
    "    first_url = ''\n",
    "    spliter = Twitter()\n",
    "    \n",
    "    #dict에 있는 article_text를 저장하기위해 만든 변수\n",
    "    num = 0\n",
    "    \n",
    "    while 1 == 1:\n",
    "        url = 'http://news.naver.com/main/list.nhn?sid1=100&listType=title&mid=sec&mode=LSD&date='+str(day)+'&page='+str(page)\n",
    "        source_code = requests.get(url)\n",
    "        plain_text = source_code.text\n",
    "        soup = BeautifulSoup(plain_text,'lxml')\n",
    "        \n",
    "        for body in soup.find_all(\"a\", class_=\"nclicks(fls.list)\"):\n",
    "             \n",
    "            #db에 저장 시킬 url\n",
    "            url = body.get('href')\n",
    "            \n",
    "            #마지막 페이지에 도달할시 함수 종료\n",
    "            if first_url == url:\n",
    "                return\n",
    "            \n",
    "            #db에 저장 시킬 article title\n",
    "            title = str(body.find_all(text=True))\n",
    "            #db에 저장 시킬 명사화된 article title 하지만 넣을지 말지 고민....\n",
    "            #nouns_title = clean_text(str(spliter.nouns(title)))\n",
    "            \n",
    "            #마지막 페이지를 구별하기 위해 만듬\n",
    "            if temp_page != page:\n",
    "                first_url = url\n",
    "                temp_page += 1\n",
    "            \"\"\"\n",
    "            db에 저장시킬 함수 호출\n",
    "            db_manager(db_id, url, title, str(day))\n",
    "            print(url)\n",
    "            print(title)\n",
    "            print(nouns_title)\n",
    "            \"\"\"\n",
    "            print(url)\n",
    "            print(title)\n",
    "            #날짜 폴더가 생성되있어야 한다. 본문내용 저장 함수 호출\n",
    "            article_text(url,num,str(day))\n",
    "            \n",
    "            num += 1\n",
    "            db_id += 1\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    main(20170806)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
